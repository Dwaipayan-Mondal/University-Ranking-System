{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8802405,"sourceType":"datasetVersion","datasetId":5293622}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install requests beautifulsoup4 pandas openpyxl scholarly","metadata":{"execution":{"iopub.status.busy":"2024-06-27T14:09:08.198792Z","iopub.execute_input":"2024-06-27T14:09:08.199264Z","iopub.status.idle":"2024-06-27T14:09:37.375945Z","shell.execute_reply.started":"2024-06-27T14:09:08.199223Z","shell.execute_reply":"2024-06-27T14:09:37.374030Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.32.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.12.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.3)\nCollecting scholarly\n  Downloading scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2024.2.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\nRequirement already satisfied: arrow in /opt/conda/lib/python3.10/site-packages (from scholarly) (1.3.0)\nCollecting bibtexparser (from scholarly)\n  Downloading bibtexparser-1.4.1.tar.gz (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: deprecated in /opt/conda/lib/python3.10/site-packages (from scholarly) (1.2.14)\nCollecting fake-useragent (from scholarly)\n  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\nCollecting free-proxy (from scholarly)\n  Downloading free_proxy-1.1.1.tar.gz (5.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from scholarly) (0.27.0)\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from scholarly) (1.0.0)\nCollecting selenium (from scholarly)\n  Downloading selenium-4.22.0-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from scholarly) (0.2.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from scholarly) (4.9.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow->scholarly) (2.8.19.20240106)\nRequirement already satisfied: pyparsing>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from bibtexparser->scholarly) (3.1.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated->scholarly) (1.14.1)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from free-proxy->scholarly) (5.2.2)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->scholarly) (4.2.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->scholarly) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->scholarly) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->scholarly) (1.7.1)\nCollecting trio~=0.17 (from selenium->scholarly)\n  Downloading trio-0.25.1-py3-none-any.whl.metadata (8.7 kB)\nCollecting trio-websocket~=0.9 (from selenium->scholarly)\n  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\nCollecting websocket-client>=1.8.0 (from selenium->scholarly)\n  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: attrs>=23.2.0 in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium->scholarly) (23.2.0)\nCollecting sortedcontainers (from trio~=0.17->selenium->scholarly)\n  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\nCollecting outcome (from trio~=0.17->selenium->scholarly)\n  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium->scholarly) (1.2.0)\nCollecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->scholarly)\n  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\nDownloading scholarly-1.7.11-py3-none-any.whl (39 kB)\nDownloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\nDownloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trio-0.25.1-py3-none-any.whl (467 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\nDownloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\nDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\nDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nBuilding wheels for collected packages: bibtexparser, free-proxy\n  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bibtexparser: filename=bibtexparser-1.4.1-py3-none-any.whl size=43252 sha256=dff3785eedb2e68727e0924f39071f768efa921a06ec337ea930942340fa45ed\n  Stored in directory: /root/.cache/pip/wheels/08/c6/c3/56e639fab68d1fdbf13ea147636d9795ccdbd3c1d3178d1332\n  Building wheel for free-proxy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for free-proxy: filename=free_proxy-1.1.1-py3-none-any.whl size=5642 sha256=ecac4330b813a03f5b1ab813061c3407c30c5e4daecb0472a2cd49e526c9b23f\n  Stored in directory: /root/.cache/pip/wheels/5a/96/c7/5a434714fff4fea9a59075428b142626e0a74f8c3bf90a50d0\nSuccessfully built bibtexparser free-proxy\nInstalling collected packages: sortedcontainers, fake-useragent, wsproto, websocket-client, outcome, bibtexparser, trio, free-proxy, trio-websocket, selenium, scholarly\n  Attempting uninstall: websocket-client\n    Found existing installation: websocket-client 1.7.0\n    Uninstalling websocket-client-1.7.0:\n      Successfully uninstalled websocket-client-1.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bibtexparser-1.4.1 fake-useragent-1.5.1 free-proxy-1.1.1 outcome-1.3.0.post0 scholarly-1.7.11 selenium-4.22.0 sortedcontainers-2.4.0 trio-0.25.1 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install cloudscraper","metadata":{"execution":{"iopub.status.busy":"2024-06-27T14:10:06.025085Z","iopub.execute_input":"2024-06-27T14:10:06.025587Z","iopub.status.idle":"2024-06-27T14:10:22.741001Z","shell.execute_reply.started":"2024-06-27T14:10:06.025545Z","shell.execute_reply":"2024-06-27T14:10:22.739293Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting cloudscraper\n  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: pyparsing>=2.4.7 in /opt/conda/lib/python3.10/site-packages (from cloudscraper) (3.1.1)\nRequirement already satisfied: requests>=2.9.2 in /opt/conda/lib/python3.10/site-packages (from cloudscraper) (2.32.3)\nRequirement already satisfied: requests-toolbelt>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from cloudscraper) (0.10.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.9.2->cloudscraper) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.9.2->cloudscraper) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.9.2->cloudscraper) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.9.2->cloudscraper) (2024.2.2)\nDownloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: cloudscraper\nSuccessfully installed cloudscraper-1.2.71\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install numba cupy\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T13:24:36.687549Z","iopub.execute_input":"2024-06-25T13:24:36.688393Z","iopub.status.idle":"2024-06-25T13:24:48.839855Z","shell.execute_reply.started":"2024-06-25T13:24:36.688356Z","shell.execute_reply":"2024-06-25T13:24:48.838754Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (0.58.1)\nRequirement already satisfied: cupy in /opt/conda/lib/python3.10/site-packages (13.1.0)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba) (0.41.1)\nRequirement already satisfied: numpy<1.27,>=1.22 in /opt/conda/lib/python3.10/site-packages (from numba) (1.26.4)\nRequirement already satisfied: fastrlock>=0.5 in /opt/conda/lib/python3.10/site-packages (from cupy) (0.8.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import cloudscraper\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport numpy as np\nimport concurrent.futures\nfrom numba import cuda\n\n# Base URL of the Stanford University page on AD Scientific Index\nbase_url = \"https://www.adscientificindex.com\"\n\n# List to store all scientists' data\ndata = []\n\n# Function to parse the scientist data from a single page\ndef parse_page(soup):\n    table = soup.find('table', {'class': 'table table-striped table-bordered table-sm'})\n    if table:\n        rows = table.find_all('tr')\n        for row in rows[1:]:\n            cols = row.find_all('td')\n            parsed_cols = []\n            profile_url = \"\"\n            for col in cols:\n                if col.find('a'):\n                    a_tag = col.find('a')\n                    if 'subject=' in a_tag['href']:\n                        subject = col.find('a').text.strip()\n                        parsed_cols.append(subject)\n                    else:\n                        for sup in a_tag.find_all('sup'):\n                            sup.decompose()\n                        name = a_tag.get_text(strip=True)\n                        parsed_cols.append(name)\n                        if \"/scientist/\" in a_tag['href']:\n                            profile_url = base_url + a_tag['href']\n                else:\n                    text_parts = col.get_text(strip=True).split()\n                    parsed_cols.append(text_parts[0] if text_parts else '')\n            parsed_cols.append(profile_url)\n            if parsed_cols not in data:\n                data.append(parsed_cols)\n    else:\n        print(\"Table not found on this page\")\n\n# Function to fetch Google Scholar URL from a professor's profile page\ndef fetch_google_scholar_url(profile_url):\n    if not profile_url:\n        return ''\n    response = scraper.get(profile_url)\n    profile_soup = BeautifulSoup(response.content, 'html.parser')\n    google_scholar_div = profile_soup.find('div', class_='id')\n    if google_scholar_div:\n        a_tag = google_scholar_div.find('a', href=True, class_='google_scholar')\n        if a_tag:\n            return a_tag['href']\n    return ''\n\n# Create a cloudscraper instance\nscraper = cloudscraper.create_scraper()\n\n# Function to scrape a single page and parse it\ndef scrape_page(page):\n    url = f\"https://www.adscientificindex.com/?s={page}&country_code=in\"\n    response = scraper.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    parse_page(soup)\n\n# Loop through each page to scrape the data concurrently\npage_ranges = range(0, 20001, 50)  # Adjusted range for example, use full range as needed\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    executor.map(scrape_page, page_ranges)\n\n# Fetch Google Scholar URLs for each professor concurrently\nprofile_urls = [row[-1] for row in data]\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    google_scholar_urls = list(executor.map(fetch_google_scholar_url, profile_urls))\n\n# Add Google Scholar URLs to data\nfor i in range(len(data)):\n    data[i].append(google_scholar_urls[i])\n\n# Ensure all rows have the same number of columns\nmax_columns = max(len(row) for row in data)\nfor row in data:\n    while len(row) < max_columns:\n        row.append('')\n\n# Define column names dynamically based on the maximum number of columns found\ncolumns = [\"Column_\" + str(i) for i in range(1, max_columns - 1)]\ncolumns.append(\"Profile_URL\")\ncolumns.append(\"Google_Scholar_URL\")\n\n# Create a DataFrame and save it to a CSV file\ndf = pd.DataFrame(data, columns=columns)\ndf.to_csv('stanford_scientists_with_google_scholar_urls_0to20000.csv', index=False)\n\nprint(\"Data has been scraped and saved to 'stanford_scientists_with_google_scholar_urls.csv'\")\n\n# Simulated heavy computation using GPU (e.g., data transformation) after scraping\n@cuda.jit\ndef heavy_computation_gpu(arr, result):\n    idx = cuda.grid(1)\n    if idx < arr.size:\n        result[idx] = arr[idx] ** 2\n\n# Convert data to a NumPy array for demonstration\ndata_array = np.random.rand(len(data), max(len(row) for row in data)).flatten()\n\n# Allocate memory on the GPU\nd_data_array = cuda.to_device(data_array)\nd_result = cuda.device_array_like(d_data_array)\n\n# Define number of threads and blocks\nthreads_per_block = 256\nblocks_per_grid = (d_data_array.size + (threads_per_block - 1)) // threads_per_block\n\n# Perform heavy computation using GPU\nheavy_computation_gpu[blocks_per_grid, threads_per_block](d_data_array, d_result)\n\n# Copy result back to host\nresult = d_result.copy_to_host()\n\n# Sum the results (if needed)\nfinal_result = np.sum(result)\nprint(f\"Result of heavy computation: {final_result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T13:44:39.075381Z","iopub.execute_input":"2024-06-25T13:44:39.076316Z","iopub.status.idle":"2024-06-25T14:16:39.078942Z","shell.execute_reply.started":"2024-06-25T13:44:39.076273Z","shell.execute_reply":"2024-06-25T14:16:39.078021Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Data has been scraped and saved to 'stanford_scientists_with_google_scholar_urls.csv'\nResult of heavy computation: 91419.93258792865\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Headers to mimic a browser request\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n    \"Accept-Language\": \"en-US,en;q=0.9\",\n    \"Accept-Encoding\": \"gzip, deflate, br\",\n    \"Connection\": \"keep-alive\"\n}\n\n# Function to get profile details\ndef get_profile_details(soup):\n    profile_name = soup.find(\"div\", id=\"gsc_prf_in\").text.strip()\n    return profile_name\n\n# Function to get the research papers and journals\ndef get_research_papers(soup):\n    papers = []\n    paper_table = soup.find_all(\"tr\", class_=\"gsc_a_tr\")\n    for paper in paper_table:\n        title_elem = paper.find(\"a\", class_=\"gsc_a_at\")\n        authors_journal_elem = paper.find_all(\"div\", class_=\"gs_gray\")\n        if len(authors_journal_elem) >= 2:\n            publication = authors_journal_elem[1].text.strip()\n        else:\n            publication = \"\"\n        if title_elem:\n            title = title_elem.text.strip()\n            papers.append((title, publication))\n    return papers\n\n# Function to load all research papers by handling pagination\ndef load_all_papers(url, headers):\n    research_papers = []\n    start = 0\n    while True:\n        try:\n            response = requests.get(url + f\"&cstart={start}&pagesize=100\", headers=headers)\n            response.raise_for_status()\n        except requests.exceptions.RequestException as e:\n            print(f\"Request error: {e}\")\n            break\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        papers = get_research_papers(soup)\n        if not papers:\n            break\n        research_papers.extend(papers)\n        start += len(papers)\n        time.sleep(20)\n    return research_papers\n\n# Function to process each URL\ndef process_url(url):\n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        profile_name = get_profile_details(soup)\n        research_papers = load_all_papers(url, headers)\n        data = []\n        for paper in research_papers:\n            data.append({\n                \"Profile Name\": profile_name,\n                \"Research Paper\": paper[0],\n                \"Journal\": paper[1]\n            })\n        return data\n    except requests.exceptions.RequestException as e:\n        print(f\"Request error for {url}: {e}\")\n        return []\n\n# Load the test CSV\nfile_path = '/kaggle/input/test-10001-10500/test.csv'\ntest_df = pd.read_csv(file_path)\n\n# Prepare to collect all data\nall_data = []\n\n# Use ThreadPoolExecutor to fetch data concurrently\nwith ThreadPoolExecutor(max_workers=10) as executor:\n    future_to_url = {executor.submit(process_url, row['scholar_link']): row['scholar_link'] for index, row in test_df.iterrows()}\n    for future in as_completed(future_to_url):\n        url = future_to_url[future]\n        try:\n            data = future.result()\n            all_data.extend(data)\n        except Exception as e:\n            print(f\"Error processing {url}: {e}\")\n\n# Create a DataFrame from the collected data\nresult_df = pd.DataFrame(all_data)\n\n# Save the results to a new CSV file\nresult_df.to_csv('professors_google_scholar_profiles_upto10500.csv', index=False)\n\nprint(\"Data collection complete. Results saved to 'professors_google_scholar_profiles_fortest.csv'.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T14:10:47.418099Z","iopub.execute_input":"2024-06-27T14:10:47.418557Z","iopub.status.idle":"2024-06-27T14:42:57.217394Z","shell.execute_reply.started":"2024-06-27T14:10:47.418517Z","shell.execute_reply":"2024-06-27T14:42:57.215643Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Request error for https://scholar.google.co.th/citations?hl=en&user=12vqBQcAAAAJ: 404 Client Error: Not Found for url: https://scholar.google.co.th/citations?hl=en&user=12vqBQcAAAAJ\nError processing https://scholar.google.co.th/citations?hl=en&user=https://scholar.google.com/citations?user=dDZIkMQAAAAJ%26hl=tr: 'NoneType' object has no attribute 'text'\nRequest error for https://scholar.google.co.th/citations?hl=en&user=OKjj9XMAAAAJ: 404 Client Error: Not Found for url: https://scholar.google.co.th/citations?hl=en&user=OKjj9XMAAAAJ\nRequest error for https://scholar.google.co.th/citations?hl=en&user=BIPIe2AAAAAJ: 404 Client Error: Not Found for url: https://scholar.google.co.th/citations?hl=en&user=BIPIe2AAAAAJ\nRequest error for https://scholar.google.co.th/citations?hl=en&user=Ax3JVNkAAAAJ: 404 Client Error: Not Found for url: https://scholar.google.co.th/citations?hl=en&user=Ax3JVNkAAAAJ\nError processing https://scholar.google.co.th/citations?hl=en&user=https://scholar.google.com/citations?user=jME1uqAAAAAJ&hl=tr: 'NoneType' object has no attribute 'text'\nRequest error for https://scholar.google.co.th/citations?hl=en&user=Y2r3ayUAAAAJ: 404 Client Error: Not Found for url: https://scholar.google.co.th/citations?hl=en&user=Y2r3ayUAAAAJ\nError processing https://scholar.google.co.th/citations?hl=en&user=https://scholar.google.com/citations?user=B3osF1IAAAAJ%26hl=tr: 'NoneType' object has no attribute 'text'\nError processing https://scholar.google.co.th/citations?hl=en&user=https://scholar.google.com/citations?user=VaMVT8UAAAAJ%26hl=tr: 'NoneType' object has no attribute 'text'\nData collection complete. Results saved to 'professors_google_scholar_profiles_fortestcsv.csv'.\n","output_type":"stream"}]}]}